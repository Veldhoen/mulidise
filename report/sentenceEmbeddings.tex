Like most of the aforementioned approaches, we aim to induce multilingual word embeddings from parallel data. In order to make sure the semantic spaces for all languages are aligned, we rely solely upon the fact that sentences are aligned without using word alignments.
We introduce the \texttt{paragraph2vec} from \cite{Le2014}. Next, we show how these models can be used in a multilingual context.

\begin{figure*}\center
\begin{subfigure}{.45\textwidth}\center
\input{figures/tikz_monoling_dismem}
\caption{PV-DM}
\label{fig:p2v-DM}
\end{subfigure}
\begin{subfigure}{.45\textwidth}\center
\input{figures/tikz_monoling_dbow}
\caption{PV-DBOW}
\label{fig:p2v-DBOW}
\end{subfigure}
\caption{\texttt{paragraph2vec} models}
\end{figure*}

\subsection{\texttt{paragraph2vec}}
An efficient model to induce word embeddings from (monolingual) text is called \texttt{word2vec}  and was introduced in \cite{mikolov2013efficient}. It was extended to a version that can induce the same kind of embeddings for paragraphs: \texttt{paragraph2vec} \cite{Le2014}. A paragraph in this case can be any sequence of words, e.g. a sentence, paragraph or entire document. There are two different models to induce them, called PV-DM (dsitributed memorey) and PV-DBOW (distributed bag of words). The authors combine paragraphs obtained from both models in their experiments.

In the DM model, a \emph{paragraph vector} is used as a part of the context of each word in the sequence (figure~\ref{fig:p2v-DM}).
The hidden layer is formed by taking the average (or sum) of the sentence vector and word vectors of the context. The network tries to predict the index of the word that was left out of the context.  
This way, the paragraph vector influences the learned representations of those words in the same way that their context words do.



In the DBOW model, no word embeddings are trained. Rather, the sentence embedding is trained by trying to predict the indexes of all words that occur in the sentence (see figure~\ref{fig:p2v-DBOW}).





\subsection{Embeddings for parallel sentences}


% This paragraph representation could also be used for encouraging similarity between two bitext sentences.
% In our novel approach, we will run the algorithm from \cite{Le2014}, but using the same paragraph vector when training word vectors from parallel sentences.
% % is dat genoeg? of moet er iets bij over waarom we denken dat de alignments dan niet nodig zijn?
% The sentence representation therefore acts as a way to relate the word spaces in both languages, without using word alignments.
% % The method is general enough to allow training on more than two sentences. It also encodes more information in the vector than just bag-of-word-vector based models like Herman & Blunsum
% We hope this will create a word vector space that is trainable on both monolingual and parallel data, allowing for the mitigation of sparsity in all languages.

Our first approach consisted of running \texttt{PV-DM} from \cite{Le2014}, 
but using the same paragraph vector when training word vectors from parallel sentences. 
This means the sentences embedding is used (together with the surrounding words) to predict every word that occurs in that sentence for both languages. Note that the softmax output ranges over the vocabularies of both languages.
The error signal from that prediction determines the value of the sentence vector.
% This model is depicted in figure~\ref{f:bilingual_dm}



The second approach is using \texttt{PV-DBOW} from \cite{Le2014} on the parallel sentences.
From the embedding of a single parallel sentence representation, the network tries to predict all words that occur in the sentence in either language.
Note that no word embeddings are trained, only word indexes are predicted from the sentence embedding.
The error is propagated back to train the sentence embeddings.
This model is depicted in figure~\ref{f:bilingual_dbow}. This extension is not restricted to bilingual training: in principle, sentences in any number of languages can be trained as long as they are parallel across all languages.


Both of these methods construct usable multilingual sentence embeddings.
The multilingual \texttt{PV-DBOW} has the added benefit of being faster, because no word embeddings are trained.


\begin{figure}

\center
\input{figures/tikz_biling_dbow}
\caption{Bilingual PV-DBOW}
\label{f:bilingual_dbow}
\end{figure}


% Deze figuur is vrij groot en is iets wat we vrij snel overboord hebben gezet...
% \begin{figure*}
% \center
% \input{figures/tikz_biling_dismem}
% \caption{Bilingual PV-DM}
% \label{f:bilingual-dm}
% \end{figure*}

