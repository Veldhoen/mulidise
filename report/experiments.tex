%Reproducing existing work with more languages
%New approach: based on par2vec (2 flavors)

We conducted several experiments 

\subsection{Data}
For training the sentence and word embeddings, we use 50 000 sentences of Europarl data that are aligned across English, German, Dutch, French, Spanish, Italian, and Portuguese, unless stated otherwise. The cross-lingual sentence alignments were created by matching the English side of all pairwise aligned corpora.
All documents were tokenized and lowercased. No other preprocessing, such as stemming, was applied.


In all experiments, words that occurred fewer than five times were excluded.
The resulting vocabulary sizes in this dataset are presented  in table~\ref{t:vocabularies}.

\begin{table}
\center
\begin{tabular}{l r r}
		&Europarl 50k 	&Europarl 500k\\
English	&8377			&24403	\\	
German	&11578		&47071	\\
Dutch		&10008				\\
French	&11092				\\	
Spanish	&10865				\\
Italian		&11503				\\
Portuguese	&11101				\\
\end{tabular}
\caption{Vocabulary size for Europarl data using a rare word cut-off of 5.}
\label{t:vocabularies}
\end{table}


\subsection{Sentence embeddings from multilingual dbow}

Using our multilingual version of the paragraph2vec dbow architecture, we obtain sentence embeddings for parallel sentences. 




In order to test the quality of the multilingual sentence embeddings, we take the document representation to simply be the average of its sentence embeddings. These representations are then used to train and test the two document classification tasks.


\subsection{Word embeddings from sentence embeddings}
We explore how the resulting sentence embeddings can be used to induce word embeddings in two languages. The word embeddings are in the same space and anticipated to be aligned cross-lingually, because the sentence representations for bilingual sentences are equal.

In one setting, we define the word representation as the average of the embeddings of all sentences it occurs in. We evaluate the word embeddings that result from this.



