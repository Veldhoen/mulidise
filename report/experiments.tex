%Reproducing existing work with more languages
%New approach: based on par2vec (2 flavors)

We conducted several experiments using the multilingual \texttt{paragraph2vec} models described in~\ref{s:sentenceEmbeddings}. In this section, the training data and implementations we use are explained. We report emperical results for different experimental set-ups.



\subsection{Data}
% parallel 7x!! no pivot!!1! wow

For training the sentence and word embeddings, we use either of two subsets of the Europarl corpus.
The default training set contains 500 000 lines of Europarl data.
Another set has 50 000 lines that are sentence-aligned across English, German, Dutch, French, Spanish, Italian, and Portuguese.
These cross-lingual sentence alignments were created by matching the English side of all pairwise aligned corpora.
This means that every sentence is available in all 7 languages. This dataset can therefore be used to train the multilingual dbow model in more than two languages, without making use of a pivot language.

All documents were tokenized and lowercased. No other preprocessing, such as stemming, was applied.
All words (and sentences) are represented by vectors of length 256.
In all experiments, words that occurred fewer than five times were excluded.
The resulting vocabulary sizes in these datasets are presented  in table~\ref{t:vocabularies}.

\begin{table}
\center
\begin{tabular}{l| r r}
		&Europarl 50k 	&Europarl 500k\\\hline
English	&8377			&24403	\\	
German	&11578		&47071	\\
Dutch		&10008				\\
French	&11092				\\	
Spanish	&10865				\\
Italian		&11503				\\
Portuguese	&11101				\\
\end{tabular}
\caption{Vocabulary size for Europarl data using a rare word cut-off of 5.}
\label{t:vocabularies}
\end{table}


As a baseline, we use the majority class estimate for the RCV data, and the F1 baseline $P$ explained in section~\ref{s:evaluation}. We also compare to the performance of vectors that results from the multitask-learning approach by \cite{klementiev2012inducing} described in section~\ref{s:relatedWork}. A distribution of their word embeddings in four language pairs (German-English, Czech-English, French-English, and Spanish-English) is available on \url{http://klementiev.org/data/distrib/}. The alignments used to populate the interaction matrix are obtained from the Europarl corpus. The word embeddings of the German-English part of the data are trained on the Reuters data that also make up the RCV evaluation set. Note that both the amount and nature of this training data differs from ours. Also the length of these vectors is much smaller: 40 (instead of 256).



\subsection{Implementation}
%GENSIM tralala
{\color{red} HIER MOET NOG IETS}

\subsection{Sentence embeddings from multilingual dbow}
Using our multilingual version of the paragraph2vec dbow architecture, we obtain sentence embeddings for parallel sentences: DE-EN are German and English paired, multi are all 7 aligned languages, EN and DE are monolingually trained sentence embeddings trained with the original dbow model. In each case, the model is trained on Europarl data, for 10 epochs. We start with a learning rate ($\alpha$) of 0.025, which is decreased with 0.002 after each epoch.

 In order to evaluate the quality of the sentence embeddings, we obtain sentence representations for the (parallel) TED corpus. We use the trained model, keeping the softmax weights fixed and training the TED sentence representations for 10 iterations.

We apply the induced sentence embeddings to the document classification task. In this case, we take the document representation to simply be the average of its sentence embeddings. These representations are then used to train and test the two document classification tasks. The results on TED data are in the second column of table~\ref{t:dbow_mono_bi}.

\begin{table*}[t]
\center
\begin{tabular}{c | r|r r r r }
Sentences 		&sentence	&	\multicolumn{4}{c}{Classification [train]-[test]}	\\
trained on: 		&quality	&EN-EN	&DE-DE	&EN-DE	&DE-EN	\\\hline
EN			&.340		&.274		&.286		&.284		&.305\\
DE			&.354		&.263		&.190		&.270		&.166\\
DE-EN			&.363		&.319		&.304		&.323		&.264\\
\end{tabular}
\caption{F1 scores on TED classification task for sentence representations and word representations.}
\label{t:dbow_mono_bi}
\end{table*}



\subsection{Word embeddings from sentence embeddings}

As explained in section~\ref{s:wordEmbeddings}, we obtain word embeddings from the sentence embeddings by taking the average of all sentence embeddings the word occurs in. Note that we use the Europarl-trained sentences for this, not the TED sentences that we reported on above. Again, we use sentences trained on English and German monolingually, as well as paired, and a version with all languages. The results are reported in the four rightmost columns table~\ref{t:dbow_mono_bi}.

It is interesting to see that %.....





\begin{table}\center
\setlength\tabcolsep{2pt}
\begin{tabular}{l | r r r r}
		& \multicolumn{4}{c}{Classification [train]-[test]}	\\
		&EN-EN	&DE-DE	&EN-DE	&DE-EN	\\\hline
Majority	&.468		&.468		&.468		&.468		\\
I-matrix	&.817		&.570		&.524		&.621		\\
dbow d-e 	&.837		&.694		&.656		&.748		\\
\end{tabular}
\caption{Accuracy score on RCV evaluation task with 1000 training documents, for word representations from I-matrix training and our own models}
\label{t:dbow_bi_klemen}
\end{table}


\begin{table}\center
\setlength\tabcolsep{3pt}
\begin{tabular}{l |rr r r}
 			&vector	&RCV 			&TED		&Analogy\\
Setting		&length	&acc			&F1		&acc\\\hline
Baseline		&		&.468			&.118	 	&		\\
I-Matrix		&40		&.817			&.149		&.006\\
Paragraph e		&256		&- 			&.340		&-\\
Paragraph e-d 	&256		&- 			&.363		&-\\
Paraword e		&256		&.836			&.274		&.025	\\
Paraword e-d	&256		&.837			&.319		&.072\\
%Paraword multi 	&256		&\\	
Google News		&300		&.915			&.486		&.613\\
\end{tabular}
\caption{Monolingual (EN-EN) evaluation for various settings.
RCV Accuracy: given 1000 training examples for classification.}
\end{table}


We explore how the resulting sentence embeddings can be used to induce word embeddings in two languages. The word embeddings are in the same space and anticipated to be aligned cross-lingually, because the sentence representations for bilingual sentences are equal.

In one setting, we define the word representation as the average of the embeddings of all sentences it occurs in. We evaluate the word embeddings that result from this.



