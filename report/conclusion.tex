%Er is een verschil tussen lokale en zinscorrelatie. Zonder word alignment kun je het eerste niet doen. Wij presenteren een manier om het tweede te doen. We kunnen best goede woord-embeddings maken van zinrepresentaties. Crosslingual met parallele dbow, maar ook monolingual training. Met parallele data kunnen we vervolgens hier word embeddings van maken.el dbow trainen.

We discovered a fundamental difference between the usage of local and global correlation to establish word embeddings. Without aligning words in parallel data, there is no apparent way to introduce local correlation in cross-lingual induction of word embeddings. However, using global (sentence level) correlation, we can create embeddings that are useful for IR-related tasks. We introduced a model to train {\tt PV-DBOW} sentence representations on parallel data in any number of languages. But we also show that we can obtain word embeddings from sentence representations that are trained on another language, as long as there is parallel data available.