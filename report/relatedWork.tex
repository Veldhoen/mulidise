Some research has focused on the induction of multilingual word embeddings, using both different techniques to obtain word representations, and different approaches to the cross-lingual aspects. The evaluation methods applied also vary a lot.


\subsection{Linear Mapping}\label{s:lin}

According to Mikolov et al. \cite{mikolov2013exploiting}, the vector space of word representations in different languages are geometrically similar, because words in languages are grounded in real world concepts.
It is therefore possible to find a linear mapping 
%(rotation and scaling) 
between these vector spaces.

The approach is to first train word embeddings on large monolingual data for both languages separately, using the \texttt{word2vec} implementation. In the reported experiments, the so-called CBOW architecture is used, that predicts a word given its context in both directions. 
Notably, the authors also propose a way to include some phrases: multi-word expressions. This may prove useful for translation, as one multiple words can together express a concept that has a single word in another language.

Using a relatively small set of gold standard word translations, in this case obtained from Google Translate, a transformation matrix $W$ is searched. The training objective is to minimize the distance between words that are translations of one another.

The evaluation is performed on a test set of gold-standard word translations, again from Google Translate. The word representation in the source language is transformed using $W$, and a ranked list of the nearest words in the target language is the output. The precision at ranks 1 and 5 is reported.

% Not parallel data, but simple dictionary (Google Translate) used as gold standard. Because of this, every word has one single gold-standard translation, instead of a similarity score to several words
% Easy evaluation method = nice
% Maybe the assumption of linear translation is too strong. The interrelation of some concepts may be evident for strong real-world concepts such as cat and dog, but some concepts may be more culturally determined: friendship, politics. Also, Asian cultures have a very different understanding of time. What about nonlinear mappings: e.g. stretching some bit of the space, and shrinking another bit?

\subsection{Multitask Learning}
Distributed representations for a pair of languages are induced jointly by Klementiev and Titov \cite{klementiev2012inducing}. Words in both languages are represented in a single vector space.

The induction is treated as a multitask learning problem where each task corresponds to a single word. The training influences other tasks depending on the task-relatedness. The latter is derived from co-occurrence statistics in bilingual parallel data: the number of alignment links between  that word and its (supposed) translations. 

The word representations are induced in a neural language model architecture. 
The $n$ preceding words form the context, their representations are concatenated to form a context vector. The probability of the next word occuring is predicted from this vector. The training procedure aims to find the word representations that minimize the data (log) likelihood: 
$L(\theta) = \sum_{t=1}^T \log \hat{P}_\theta (w_t|w_{t-n+1:t-1})$. 

The method is evaluated on a real-world task: crosslingual document classification. Topic annotations are available for documents in one of the languages, and the system predicts the topics of documents in the other language. The jointly induced word representation outperform two other approaches to the problem: glossing (where every word in the document is translated separately, based on word alignments) and Machine Translation.


\subsection{Joint Learning from Sentence Embeddings}
Unlike the previous approaches, Hermann and Blunsum \cite{hermann2013multilingual} start from sentence alignments, which share the same semantics.
The assumption is that some function can describe the composition of word embeddings into a sentence embedding.
For the sake of argument, the authors use a simple bag-of-words additive interpretation of composition. 
The word embeddings are induced jointly for both languages from these sentence-embeddings, by minimizing the distance between both sums of word embeddings.
% However, the sentence embeddings are not present a priori, nor is the training aimed at obtaining these.
% ? the sentence embeddings are not learned? 
% Nope, just the fact that two sentences should have the same semantics is used.
In order to make sure the weights won't be reduced to zero, similarity between unaligned sentence embeddings is penalized.
%This method is implemented in a software package named \texttt{bicvm}.

The same evaluation as in \cite{klementiev2012inducing} is applied, i.e. the document classification task. Furthermore, the authors present a graphical qualitative analysis. In \cite{Hermann2014}, one of the authors expands this approach by evaluating on a larger number of language pairs.

% The method works surprisingly well, given the poor composition they assume.
% The authors claim it is possible to extend this approach to document-aligned or even unaligned data, but I doubt that

%\subsection{Comparison}
%The first approach learns word embeddings from a large monolingual corpus and creates a mapping later, while the second two approaches learn the embeddings and language correspondance jointly. 

%The first two approaches obtain language correspondence from word similarity, either from Google Translate or based on alignments, which results in a weighted similarity score. The latter induces this correspondence from sentence equivalence, assuming an addition for compositional semantics.
