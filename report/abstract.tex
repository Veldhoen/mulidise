\begin{abstract}

By representing the meaning of language in a vector space, we can model many semantic phenomena in a natural way.
These representations have also proven useful for many Natural Language Processing tasks.
We propose a method for using multilingual sentence embeddings to construct word embeddings in a shared vector space.
We also highlight the difference between approaches that rely on local word context and on co-occurrence within sentences.
Our model learns word embeddings that share features across multiple languages.
We evaluate our word embeddings on a cross-lingual document classification task on two corpora.


\end{abstract}
