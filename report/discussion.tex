%- misschien iets over het feit dat Duits moeilijk is, en dat we geen preprocessing doen.
%- mogelijkheid andere modellen voor woorden uit zinnen (gewogen, CVM model erin plakken Ã  la Hermann&Blunsom)
%- Probleem van word senses, en hoe daarvoor parallel data gebruikt kan worden

Our experiments are somewhat tentative, given the scope of this research. We discuss some considerations and directions for more thorough research.

\subsection{Word senses}
Many approaches to word embeddings assume that each word gets mapped to a single embedding. However, words often posses several senses: different meanings that may be more or less related. In order to improve the quality of semantic spaces, these word senses should be taken into account. Parallel data might actually help to induce word senses, since languages can actually have different words where another language uses the same word for different senses.

\subsection{Issues of morpology}
Of course, languages differ in what it means to be a `word'. English has quite simple morphology, while Latin languages often use inflections and German and Dutch have a lot of compound words. Simple tokenization of text may thus not produce equally reliable vocabularies for different languages, and therefore distort performance. A possible solution is to apply stemming or other morphological decomposition. We have not looked into these possibilities, but it would sure be a good idea to take this in consideration in further research on crosslingual embeddings.

\subsection{Models of composition}
In our experiments, we used a bag-of-words representation of the sentences in the training of sentence embeddings. In the estimation of word embeddings, we assume a reversed additive compositional model: we take the word to be the average of all sentences it occurs in. A lot of research has focused at more pregnant models of composition, ranging from a bigram additive model \cite{hermann2014multilingual} to complex syntactically inspired constitutionality. Given a sentence representation, whether it is trained with {\tt PV-DBOW} or otherwise, any model of composition could in principle be used to obtain word embeddings.